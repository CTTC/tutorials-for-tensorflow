{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main methods of getting data into a TensorFlow program:\n",
    "\n",
    "* **Feeding**: Python code provides the data when running each step.\n",
    "* **Preloaded data**: a constant or variable in the TensorFlow graph holds all the data (for small data sets).\n",
    "* **Reading from files**: an input pipeline reads the data from files at the beginning of a TensorFlow graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's feed mechanism:\n",
    "* Inject data into any Tensor in a computation graph\n",
    "* Temporarily replace the output of an operation with a tensor value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you can replace any Tensor with feed data, including variables and constants, the best practice is to use a [placeholder op](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#placeholder) node. A placeholder exists solely to serve as the target of feeds. \n",
    "\n",
    "Feeding is great for small examples and easily interacting with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feeding Example I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 14.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = tf.mul(input1, input2)\n",
    "with tf.Session() as sess:\n",
    "     print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feeding Example II - MNIST \n",
    "Full code is available at: [fully_connected_feed.py](https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/examples/tutorials/mnist/fully_connected_feed.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(batch_size, mnist.IMAGE_PIXELS))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    return images_placeholder, labels_placeholder\n",
    "    \n",
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "    images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "    feed_dict = {\n",
    "        images_pl: images_feed,\n",
    "        labels_pl: labels_feed,\n",
    "    }\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "    with tf.Graph().as_default():\n",
    "        images_placeholder, labels_placeholder = placeholder_inputs(FLAGS.batch_size)\n",
    "        #...\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "        for step in xrange(FLAGS.max_steps):\n",
    "            feed_dict = fill_feed_dict(data_sets.train, images_placeholder, labels_placeholder)\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preloaded Data\n",
    "This is only used for small data sets that can be loaded entirely in memory. There are two approaches:\n",
    "\n",
    "* Store the data in a constant.\n",
    "* Store the data in a variable, that you initialize and then never change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Load images in python\n",
    "##### Using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "data_dir = '/home/chentao/Pictures/'\n",
    "filename_list = glob.glob('%s*.jpg' % (data_dir))\n",
    "filename_queue = tf.train.string_input_producer(filename_list) \n",
    "reader = tf.WholeFileReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "img = tf.image.decode_jpeg(value) # use png or jpg decoder based on your files.\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init_op)\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "for i in range(len(filename_list)):  # length of your filename list\n",
    "    image = img.eval()  # here is your image Tensor\n",
    "    Image.fromarray(np.asarray(image)).show()\n",
    "\n",
    "coord.request_stop()\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "data_dir = '/home/chentao/Pictures/'\n",
    "for filename in glob.glob('%s*.jpg' % (data_dir)):\n",
    "    image = cv2.imread(filename)\n",
    "    # Only for display\n",
    "    cv2.imshow(filename, image)\n",
    "    cv2.waitKey(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "data_dir = '/home/chentao/Pictures/'\n",
    "for filename in glob.glob('%s*.jpg' % (data_dir)):\n",
    "    image = np.asarray(Image.open(filename))\n",
    "    Image.fromarray(image).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skimage.io as ski_io\n",
    "from PIL import Image\n",
    "import glob\n",
    "data_dir = '/home/chentao/Pictures/'\n",
    "for filename in glob.glob('%s*.jpg' % (data_dir)):\n",
    "    image = ski_io.imread(filename)\n",
    "    Image.fromarray(image).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "import glob\n",
    "data_dir = '/home/chentao/Pictures/'\n",
    "for filename in glob.glob('%s*.jpg' % (data_dir)):\n",
    "    # image = misc.imread(filename) # either one\n",
    "    image = ndimage.imread(filename)\n",
    "    Image.fromarray(image).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Images into NHWC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "data_dir = '/home/chentao/Pictures/'\n",
    "images = []\n",
    "for filename in glob.glob('%s*.jpg'%(data_dir)):\n",
    "    image = np.asarray(Image.open(filename))\n",
    "    images.append(image)\n",
    "## Method I\n",
    "images = tf.pack(images, axis=0)\n",
    "# show the first image\n",
    "sess = tf.InteractiveSession()\n",
    "Image.fromarray(images[0].eval()).show()\n",
    "## Method II\n",
    "images = np.array(images)\n",
    "Image.fromarray(images[0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate multiple images on the channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "data_dir = '/home/chentao/Pictures/'\n",
    "images = []\n",
    "for filename in glob.glob('%s*.jpg'%(data_dir)):\n",
    "    image = np.asarray(Image.open(filename))\n",
    "    images.append(image)\n",
    "## Method I\n",
    "images = tf.concat(2, images)\n",
    "sess = tf.InteractiveSession()\n",
    "Image.fromarray(images[:,:,:3].eval()).show()\n",
    "## Method II\n",
    "images = np.concatenate(images, axis=2)\n",
    "Image.fromarray(images[:,:,:3]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Preloaded Data - Using Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_images = #...\n",
    "training_labels = #...\n",
    "with tf.Session():\n",
    "    input_images = tf.constant(training_images)\n",
    "    input_labels = tf.constant(training_labels)\n",
    "    #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preloaded Data - Using Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_images = #...\n",
    "training_labels = #...\n",
    "with tf.Session() as sess:\n",
    "    images_initializer = tf.placeholder(dtype=training_images.dtype,\n",
    "                                        shape=training_images.shape)\n",
    "    label_initializer = tf.placeholder(dtype=training_labels.dtype,\n",
    "                                       shape=training_labels.shape)\n",
    "    input_images = tf.Variable(images_initializer, trainable=False,\n",
    "                               collections=[])\n",
    "    input_labels = tf.Variable(label_initializer, trainable=False,\n",
    "                               collections=[])\n",
    "    #...\n",
    "    sess.run(input_images.initializer, feed_dict={images_initializer: training_images})\n",
    "    sess.run(input_labels.initializer, feed_dict={label_initializer: training_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preloaded Data - Generate Batch\n",
    "Full code is available at [fully_connected_preloaded.py](https://www.tensorflow.org/code/tensorflow/examples/how_tos/reading_data/fully_connected_preloaded.py) and [fully_connected_preloaded_var.py](https://www.tensorflow.org/code/tensorflow/examples/how_tos/reading_data/fully_connected_preloaded_var.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image, label = tf.train.slice_input_producer([input_images, input_labels],    \n",
    "                                             num_epochs=FLAGS.num_epochs,\n",
    "                                             shuffle=True)\n",
    "label = tf.cast(label, tf.int32)\n",
    "images, labels = tf.train.batch([image, label], \n",
    "                                batch_size=FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from Files\n",
    "A typical pipeline for reading records from files has the following stages:\n",
    "\n",
    "* The list of filenames\n",
    "* Optional filename shuffling\n",
    "* Optional epoch limit\n",
    "* Filename queue\n",
    "* A Reader for the file format\n",
    "* decoder for a record read by the reader\n",
    "* Optional preprocessing\n",
    "* Example queue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To acquire the list of filenames, use either a constant string Tensor (like `[\"file0\", \"file1\"] or [(\"file%d\" % i) for i in range(2)]`) or use either [glob.glob()](https://docs.python.org/2/library/glob.html) or [tf.train.match_filenames_once()](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#match_filenames_once).\n",
    "\n",
    "Pass the list of filenames to the [tf.train.string_input_producer](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#string_input_producer), string_input_producer creates a FIFO queue for holding the filenames until the reader needs them.\n",
    "\n",
    "Select the [reader](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#readers) that matches your input file format and pass the filename queue to the reader's read method. The read method outputs a key identifying the file and record (useful for debugging if you have some weird records), and a scalar string value. Use one (or more) of the decoder and conversion ops to decode this string into the tensors that make up an [example](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#example-protocol-buffer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV Files\n",
    "Two key functions:\n",
    "* [tf.TextLineReader()](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#TextLineReader)\n",
    "* [tf.decode_csv()](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#decode_csv)\n",
    "\n",
    "Remember to call [tf.train.start_queue_runners()](https://www.tensorflow.org/versions/r0.12/api_docs/python/train.html#start_queue_runners) to populate the queue before you call run or eval to execute the read. Otherwise read will block while it waits for filenames from the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "filename_queue = tf.train.string_input_producer(['csv_data/file0.csv',             \n",
    "                                                 'csv_data/file1.csv'])\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "record_defaults = [[1], [1], [1], [1], [1]]\n",
    "col1, col2, col3, col4, col5 = tf.decode_csv(\n",
    "                                  value, record_defaults=record_defaults)\n",
    "features = tf.pack([col1, col2, col3, col4])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Start populating the filename queue.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(2000):\n",
    "        key_in,value_in,example,label = sess.run([key, value, features, col5])\n",
    "        print('key:',key_in, ' value:',value_in, '  example:',list(example), ' label:',label)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed Length Records\n",
    "To read binary files in which each record is a fixed number of bytes, use [tf.FixedLengthRecordReader](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#FixedLengthRecordReader) with the [tf.decode_raw](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#decode_raw) operation.\n",
    "\n",
    "Full code is available at:  [cifar10_input.py](https://www.tensorflow.org/code/tensorflow/models/image/cifar10/cifar10_input.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in xrange(1, 6)]\n",
    "filename_queue = tf.train.string_input_producer(filenames)\n",
    "reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "key, value = reader.read(filename_queue)\n",
    "record_bytes = tf.decode_raw(value, tf.uint8)\n",
    "\n",
    "# Suppose the first bytes represent the label\n",
    "label = tf.cast(tf.slice(record_bytes, [0], [label_bytes]), tf.int32)\n",
    "\n",
    "# Suppose the remaining bytes after the label represent the image, \n",
    "# which we reshape from [depth * height * width] to [depth, height, width].\n",
    "depth_major = tf.reshape(tf.slice(record_bytes, [label_bytes], [image_bytes]), [depth, height, width])\n",
    "\n",
    "# Convert from [depth, height, width] to [height, width, depth].\n",
    "image = tf.transpose(depth_major, [1, 2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard TensorFlow format\n",
    "Convert whatever data into a supported format (**[TFRecords File](https://www.tensorflow.org/versions/r0.10/api_docs/python/python_io.html#tfrecords-format-details)**)\n",
    "\n",
    "A TFRecords file contains [tf.train.Example protocol buffers](https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/core/example/example.proto), which contain [Features](https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/core/example/feature.proto) as a field. Basically, an **Example** always contains **Features**. **Features** contains a **map** of strings to **Feature**. And finally, a **Feature** contains one of a **FloatList**, a **ByteList** or a **Int64List**.\n",
    "\n",
    "To convert your data into TFRecord file, you first write a little program that gets your data, then stuffs it in an **Example** protocol buffer, **serializes** the protocol buffer to a string, and then **writes** the string to a TFRecords file using the [tf.python_io.TFRecordWriter](https://www.tensorflow.org/versions/r0.12/api_docs/python/python_io.html#TFRecordWriter) class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What an example looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct the Example proto boject\n",
    "example = tf.train.Example(\n",
    "    # Example contains a Features proto object\n",
    "    features=tf.train.Features(\n",
    "        # Features contains a map of string to Feature proto objects\n",
    "        feature={\n",
    "        # A Feature contains one of either a int64_list,\n",
    "        # float_list, or bytes_list\n",
    "        'label': tf.train.Feature(int64_list = tf.train.Int64List(value = [label])),\n",
    "        'image': tf.train.Feature(int64_list = tf.train.Int64List(value = features.astype('int64'))),\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to TFRecord\n",
    "Full code is available at [convert_to_records.py](https://www.tensorflow.org/code/tensorflow/examples/how_tos/reading_data/convert_to_records.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list = tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list = tf.train.BytesList(value=[value]))\n",
    "\n",
    "filename = 'data.tfrecords'\n",
    "writer = tf.python_io.TFRecordWriter(filename)\n",
    "for index in range(dataset_size):\n",
    "    image_raw = images[index].tostring()\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={'height': _int64_feature(rows),\n",
    "                                                                   'width': _int64_feature(cols),\n",
    "                                                                   'depth': _int64_feature(depth),\n",
    "                                                                   'label': _int64_feature(int(labels[index])),\n",
    "                                                                   'image_raw': _bytes_feature(image_raw)}))\n",
    "    writer.write(example.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read TFRecord Method I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "filename = \"data.tfrecords\"\n",
    "for serialized_example in tf.python_io.tf_record_iterator(filename):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(serialized_example)\n",
    "\n",
    "    # traverse the Example format to get data\n",
    "    image = example.features.feature['image_raw'].int64_list.value\n",
    "    label = example.features.feature['label'].int64_list.value[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read TFRecord Method II\n",
    "Three key functions:\n",
    "* [tf.TFRecordReader()](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#TFRecordReader)\n",
    "* [tf.parse_single_example()](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#parse_single_example)\n",
    "* [tf.decode_raw()](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#decode_raw)\n",
    "\n",
    "It is important to remember that TensorFlow’s graphs contain **state**. It is this state that allows the **TFRecordReader** to remember the location of the **tfrecord** it’s reading and always return the next one. This is why for almost all TensorFlow work we need to initialize the graph. We can use the helper function **tf.global_variables_initializer()**, which constructs an op that initializes the state on the graph when you run it.\n",
    "\n",
    "Full code is available at [fully_connected_reader.py](https://www.tensorflow.org/code/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"data.tfrecords\"\n",
    "filename_queue = tf.train.string_input_producer([filename], num_epochs=num_epochs)\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(serialized_example,\n",
    "                                   features={'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                                             'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                   })\n",
    "image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "label = tf.cast(features['label'], tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing\n",
    "You can then do any preprocessing of these examples you want. This would be any processing that doesn't depend on trainable parameters. Examples include normalization of your data, picking a random slice, adding noise or distortions, etc. \n",
    "\n",
    "Full code is available at [cifar10_input.py](https://www.tensorflow.org/code/tensorflow/models/image/cifar10/cifar10_input.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batching\n",
    "At the end of the pipeline we use another queue to batch together examples for training, evaluation, or inference.\n",
    "\n",
    "Note: \n",
    "     * tf.train.shuffle_batch(tensors,enqueue_many=False): tensors is a single example\n",
    "     * tf.train.shuffle_batch(tensors,enqueue_many=True): tensors is a batch of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_my_file_format(filename_queue):\n",
    "    reader = tf.SomeReader()\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    example, label = tf.some_decoder(record_string)\n",
    "    processed_example = some_processing(example)\n",
    "    return processed_example, label\n",
    "# Method 1\n",
    "# filenames is a list of image files like ['data/1.jpg', 'data/2.jpg',...]\n",
    "# or a list of tfrecord files, csv files, binary files, etc.\n",
    "\n",
    "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs, shuffle=True)\n",
    "    example, label = read_my_file_format(filename_queue)\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch([example, label], \n",
    "                                                        batch_size=batch_size, \n",
    "                                                        capacity=capacity,\n",
    "                                                        min_after_dequeue=min_after_dequeue)\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method 2\n",
    "# filenames is a list of filenames like ['data/1.jpg', 'data/2.jpg',...]\n",
    "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
    "    image_files = tf.convert_to_tensor(all_filenames, dtype=dtypes.string)\n",
    "    labels = tf.convert_to_tensor(all_labels, dtype=dtypes.int32)\n",
    "    \n",
    "    train_input_queue = tf.train.slice_input_producer([image_files, labels],\n",
    "                                                      shuffle=True)\n",
    "    file_content = tf.read_file(train_input_queue[0])\n",
    "    train_image = tf.image.decode_jpeg(file_content, channels=NUM_CHANNELS)\n",
    "    train_label = train_input_queue[1]\n",
    "    train_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "    \n",
    "    train_image_batch, train_label_batch = tf.train.batch([train_image, train_label],\n",
    "                                                          batch_size=BATCH_SIZE)\n",
    "    return train_image_batch, train_label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prefetch by QueueRunner\n",
    "many of the **tf.train** functions used above add [QueueRunner](https://www.tensorflow.org/versions/r0.12/api_docs/python/train.html#QueueRunner) objects to your graph. These require that you call [tf.train.start_queue_runners](https://www.tensorflow.org/versions/r0.12/api_docs/python/train.html#start_queue_runners) before running any training or inference steps, or it will hang forever. This will start threads that run the input pipeline, filling the example queue so that the dequeue to get the examples will succeed. This is best combined with a [tf.train.Coordinator](https://www.tensorflow.org/versions/r0.12/api_docs/python/train.html#Coordinator) to cleanly shut down these threads when there are errors. If you set a limit on the number of epochs, that will use an epoch counter that will need to be initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "# Start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        # Run training steps or whatever\n",
    "        sess.run(train_op)\n",
    "\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Done training -- epoch limit reached')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sparse input data\n",
    "SparseTensors don't play well with queues. If you use SparseTensors you have to decode the string records using [tf.parse_example](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#parse_example) after batching (instead of using [tf.parse_single_example](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html#parse_single_example) before batching).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In conclusion\n",
    "\n",
    "First we create the graph. It will have a few pipeline stages that are connected by queues. \n",
    "\n",
    "The first stage will generate filenames to read and enqueue them in the filename queue. \n",
    "\n",
    "The second stage consumes filenames (using a Reader), produces examples, and enqueues them in an example queue. Depending on how you have set things up, you may actually have a few independent copies of the second stage, so that you can read from multiple files in parallel. \n",
    "\n",
    "At the end of these stages is an enqueue operation, which enqueues into a queue that the next stage dequeues from. We want to start threads running these enqueuing operations, so that our training loop can dequeue examples from the example queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [TensorFlow Data Input (Part 1): Placeholders, Protobufs & Queues](https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/)\n",
    "* [TensorFlow Reading Data](https://www.tensorflow.org/versions/r0.12/how_tos/reading_data/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
